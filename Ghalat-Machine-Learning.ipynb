{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Ghalat Machine Learning!\n",
      "\n",
      "All models are set to train\n",
      "         Have a tea and leave everything on us ;-)\n",
      "************************************************************ \n",
      "Successfully dealt with missing data!\n",
      "\n",
      "X:\n",
      "\n",
      "        0    1    2    3\n",
      "0    5.1  3.5  1.4  0.2\n",
      "1    4.9  3.0  1.4  0.2\n",
      "2    4.7  3.2  1.3  0.2\n",
      "3    4.6  3.1  1.5  0.2\n",
      "4    5.0  3.6  1.4  0.2\n",
      "..   ...  ...  ...  ...\n",
      "145  6.7  3.0  5.2  2.3\n",
      "146  6.3  2.5  5.0  1.9\n",
      "147  6.5  3.0  5.2  2.0\n",
      "148  6.2  3.4  5.4  2.3\n",
      "149  5.9  3.0  5.1  1.8\n",
      "\n",
      "[150 rows x 4 columns] \n",
      "Test Data:\n",
      "\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      " ************************************************************\n",
      "\n",
      " ************************************************************ \n",
      "Successfully encoded categorical data with Target Mean Encoding using Stratified KFolds technique!\n",
      "\n",
      " X:\n",
      "\n",
      "        0    1    2    3\n",
      "0    5.1  3.5  1.4  0.2\n",
      "1    4.9  3.0  1.4  0.2\n",
      "2    4.7  3.2  1.3  0.2\n",
      "3    4.6  3.1  1.5  0.2\n",
      "4    5.0  3.6  1.4  0.2\n",
      "..   ...  ...  ...  ...\n",
      "145  6.7  3.0  5.2  2.3\n",
      "146  6.3  2.5  5.0  1.9\n",
      "147  6.5  3.0  5.2  2.0\n",
      "148  6.2  3.4  5.4  2.3\n",
      "149  5.9  3.0  5.1  1.8\n",
      "\n",
      "[150 rows x 4 columns] \n",
      "\n",
      "Test Data:\n",
      "\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      " ************************************************************\n",
      "\n",
      " ************************************************************ \n",
      " Generating new features !\n",
      " ************************************************************\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 406 features.\n",
      "[AutoFeat] With 150 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 24 transformed features from 4 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 366 feature combinations from 378 original feature tuples - done.\n",
      "[feateng] Generated altogether 390 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 112 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(96, 150), dtype=float32).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(150,), dtype=int64).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(96, 150), dtype=float32).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(150,), dtype=int64).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(96, 150), dtype=float32).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(150,), dtype=int64).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(96, 150), dtype=float32).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(150,), dtype=int64).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(96, 150), dtype=float32).\n",
      "Pickling array (shape=(96,), dtype=object).\n",
      "Pickling array (shape=(150,), dtype=int64).\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   12.8s remaining:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   13.2s remaining:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   18.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   18.2s finished\n",
      "[featsel] 13 features after 5 feature selection runs\n",
      "[featsel] 6 features after correlation filtering\n",
      "[featsel] 6 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 9 feature columns (5 new).\n",
      "[AutoFeat] Training final classification model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "[ 10.20348775  16.11038054 -26.31386825]\n",
      "22.593955 * sqrt(x2)/x0\n",
      "6.885691 * exp(x3)/x2\n",
      "6.094648 * exp(x3)/x3\n",
      "3.880116 * 2\n",
      "3.838882 * log(x2)/x2\n",
      "0.882875 * x2**3*x3**2\n",
      "[AutoFeat] Final score: 0.9733\n",
      "\n",
      " ************************************************************ \n",
      "Successfully generated new features! and selected the best features\n",
      "\n",
      " X:\n",
      "\n",
      "        0    1    2    3  exp(x3)/x2  sqrt(x2)/x0  x2**3*x3**2  log(x2)/x2  \\\n",
      "0    5.1  3.5  1.4  0.2    0.872431     0.232003      0.10976    0.240337   \n",
      "1    4.9  3.0  1.4  0.2    0.872431     0.241473      0.10976    0.240337   \n",
      "2    4.7  3.2  1.3  0.2    0.939541     0.242591      0.08788    0.201819   \n",
      "3    4.6  3.1  1.5  0.2    0.814269     0.266249      0.13500    0.270310   \n",
      "4    5.0  3.6  1.4  0.2    0.872431     0.236643      0.10976    0.240337   \n",
      "..   ...  ...  ...  ...         ...          ...          ...         ...   \n",
      "145  6.7  3.0  5.2  2.3    1.918112     0.340351    743.81632    0.317050   \n",
      "146  6.3  2.5  5.0  1.9    1.337179     0.354931    451.25000    0.321888   \n",
      "147  6.5  3.0  5.2  2.0    1.420972     0.350823    562.43200    0.317050   \n",
      "148  6.2  3.4  5.4  2.3    1.847071     0.374805    832.98456    0.312296   \n",
      "149  5.9  3.0  5.1  1.8    1.186205     0.382766    429.78924    0.319459   \n",
      "\n",
      "     exp(x3)/x3  \n",
      "0      6.107014  \n",
      "1      6.107014  \n",
      "2      6.107014  \n",
      "3      6.107014  \n",
      "4      6.107014  \n",
      "..          ...  \n",
      "145    4.336601  \n",
      "146    3.518892  \n",
      "147    3.694528  \n",
      "148    4.336601  \n",
      "149    3.360915  \n",
      "\n",
      "[150 rows x 9 columns] \n",
      "\n",
      "Test Data:\n",
      "\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      " ************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris(return_X_y=True)\n",
    "\n",
    "#AUTO Feature Engineering\n",
    "from GML.Ghalat_Machine_Learning import Ghalat_Machine_Learning\n",
    "gml = Ghalat_Machine_Learning()\n",
    "\n",
    "new_X,y = gml.Auto_Feature_Engineering(X,y,type_of_task='Classification',test_data=None,\n",
    "                                                          splits=6,fill_na_='median',ratio_drop=0.2,\n",
    "                                                          generate_features=True,feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  LogisticRegressionCV  got validation accuracy of  0.9555555555555556\n",
      "Model  LogisticRegression  got validation accuracy of  0.9777777777777777\n",
      "Model  SVC  got validation accuracy of  0.9777777777777777\n",
      "Model  DecisionTreeClassifier  got validation accuracy of  0.9333333333333333\n",
      "Model  KNeighborsClassifier  got validation accuracy of  0.9555555555555556\n",
      "Model  SGDClassifier  got validation accuracy of  0.9777777777777777\n",
      "Model  RandomForestClassifier  got validation accuracy of  0.9333333333333333\n",
      "Model  AdaBoostClassifier  got validation accuracy of  0.9333333333333333\n",
      "Model  ExtraTreesClassifier  got validation accuracy of  0.9555555555555556\n",
      "Model  XGBClassifier  got validation accuracy of  0.9333333333333333\n",
      "Model  LGBMClassifier  got validation accuracy of  0.9555555555555556\n",
      "Model  CatBoostClassifier  got validation accuracy of  0.9555555555555556\n",
      "Model  GradientBoostingClassifier  got validation accuracy of  0.9333333333333333\n",
      "Model  NaiveBayesGaussian  got validation accuracy of  0.9777777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0415 23:31:05.575490  9628 deprecation.py:506] From C:\\Users\\Chien Hua Lo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  MLPClassifier  got validation accuracy of  0.9555555555555556\n",
      "\n",
      " **************************************** \n",
      "Training Neural Network\n",
      " ****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0415 23:31:05.919962  9628 deprecation.py:323] From C:\\Users\\Chien Hua Lo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0415 23:31:06.543457  9628 module_wrapper.py:139] From C:\\Users\\Chien Hua Lo\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network got validation accuracy of  0.9777777777777777\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               2560      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 43,907\n",
      "Trainable params: 43,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      " ************************************************************ \n",
      "Round One Results\n",
      " ************************************************************ \n",
      "                         Model  Val_Accuracy  CV on 5 folds\n",
      "0        LogisticRegressionCV      0.955556       0.980000\n",
      "0              Neural Network      0.977778       0.977778\n",
      "0          LogisticRegression      0.977778       0.973333\n",
      "0        KNeighborsClassifier      0.955556       0.973333\n",
      "0               SGDClassifier      0.977778       0.973333\n",
      "0      DecisionTreeClassifier      0.933333       0.966667\n",
      "0      RandomForestClassifier      0.933333       0.966667\n",
      "0          AdaBoostClassifier      0.933333       0.966667\n",
      "0        ExtraTreesClassifier      0.955556       0.966667\n",
      "0          CatBoostClassifier      0.955556       0.966667\n",
      "0          NaiveBayesGaussian      0.977778       0.966667\n",
      "0               MLPClassifier      0.955556       0.966667\n",
      "0                         SVC      0.977778       0.966667\n",
      "0  GradientBoostingClassifier      0.933333       0.960000\n",
      "0               XGBClassifier      0.933333       0.960000\n",
      "0              LGBMClassifier      0.955556       0.960000 \n",
      " ************************************************************\n",
      "Model  LogisticRegressionCV  got validation accuracy of  0.9555555555555556\n",
      "Model  Neural Network  got validation accuracy of  0.9333333333333333\n",
      "Model  LogisticRegression  got validation accuracy of  0.9777777777777777\n",
      "Model  KNeighborsClassifier  got validation accuracy of  0.9555555555555556\n",
      "Model  SGDClassifier  got validation accuracy of  0.9333333333333333\n",
      "\n",
      " ************************************************************ \n",
      "Round Two Results\n",
      " ************************************************************ \n",
      "                   Model  Val_Accuracy  CV on 5 folds\n",
      "0  LogisticRegressionCV      0.955556       0.980000\n",
      "0         SGDClassifier      0.933333       0.980000\n",
      "0    LogisticRegression      0.977778       0.973333\n",
      "0  KNeighborsClassifier      0.955556       0.973333\n",
      "0        Neural Network      0.933333       0.933333 \n",
      " ************************************************************\n",
      "\n",
      "\n",
      " **************************************** \n",
      "Suggested Models for Stacking\n",
      " **************************************** \n",
      " 0    LogisticRegressionCV\n",
      "0           SGDClassifier\n",
      "0      LogisticRegression\n",
      "Name: Model, dtype: object\n",
      "**************************************** \n",
      " PLEASE NOTE: these results are calculated using  <function accuracy_score at 0x0000024B7EA5D4C8>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "best_model = gml.GMLClassifier(new_X,y,neural_net='Yes',epochs=100,models=[MLPClassifier()],verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=1000, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                     random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Ghalat Machine Learning!\n",
      "\n",
      "All models are set to train\n",
      "         Have a tea and leave everything on us ;-)\n",
      "************************************************************ \n",
      "Successfully dealt with missing data!\n",
      "\n",
      "X:\n",
      "\n",
      "            0     1      2    3      4      5     6       7    8      9    10  \\\n",
      "0    0.00632  18.0   2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
      "1    0.02731   0.0   7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
      "2    0.02729   0.0   7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
      "3    0.03237   0.0   2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
      "4    0.06905   0.0   2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
      "..       ...   ...    ...  ...    ...    ...   ...     ...  ...    ...   ...   \n",
      "501  0.06263   0.0  11.93  0.0  0.573  6.593  69.1  2.4786  1.0  273.0  21.0   \n",
      "502  0.04527   0.0  11.93  0.0  0.573  6.120  76.7  2.2875  1.0  273.0  21.0   \n",
      "503  0.06076   0.0  11.93  0.0  0.573  6.976  91.0  2.1675  1.0  273.0  21.0   \n",
      "504  0.10959   0.0  11.93  0.0  0.573  6.794  89.3  2.3889  1.0  273.0  21.0   \n",
      "505  0.04741   0.0  11.93  0.0  0.573  6.030  80.8  2.5050  1.0  273.0  21.0   \n",
      "\n",
      "         11    12  \n",
      "0    396.90  4.98  \n",
      "1    396.90  9.14  \n",
      "2    392.83  4.03  \n",
      "3    394.63  2.94  \n",
      "4    396.90  5.33  \n",
      "..      ...   ...  \n",
      "501  391.99  9.67  \n",
      "502  396.90  9.08  \n",
      "503  396.90  5.64  \n",
      "504  393.45  6.48  \n",
      "505  396.90  7.88  \n",
      "\n",
      "[506 rows x 13 columns] \n",
      "Test Data:\n",
      "\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      " ************************************************************\n",
      "\n",
      " ************************************************************ \n",
      "Successfully encoded categorical data with Target Mean Encoding using Stratified KFolds technique!\n",
      "\n",
      " X:\n",
      "\n",
      "            0     1      2    3      4      5     6       7    8      9    10  \\\n",
      "0    0.00632  18.0   2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
      "1    0.02731   0.0   7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
      "2    0.02729   0.0   7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
      "3    0.03237   0.0   2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
      "4    0.06905   0.0   2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
      "..       ...   ...    ...  ...    ...    ...   ...     ...  ...    ...   ...   \n",
      "501  0.06263   0.0  11.93  0.0  0.573  6.593  69.1  2.4786  1.0  273.0  21.0   \n",
      "502  0.04527   0.0  11.93  0.0  0.573  6.120  76.7  2.2875  1.0  273.0  21.0   \n",
      "503  0.06076   0.0  11.93  0.0  0.573  6.976  91.0  2.1675  1.0  273.0  21.0   \n",
      "504  0.10959   0.0  11.93  0.0  0.573  6.794  89.3  2.3889  1.0  273.0  21.0   \n",
      "505  0.04741   0.0  11.93  0.0  0.573  6.030  80.8  2.5050  1.0  273.0  21.0   \n",
      "\n",
      "         11    12  \n",
      "0    396.90  4.98  \n",
      "1    396.90  9.14  \n",
      "2    392.83  4.03  \n",
      "3    394.63  2.94  \n",
      "4    396.90  5.33  \n",
      "..      ...   ...  \n",
      "501  391.99  9.67  \n",
      "502  396.90  9.08  \n",
      "503  396.90  5.64  \n",
      "504  393.45  6.48  \n",
      "505  396.90  7.88  \n",
      "\n",
      "[506 rows x 13 columns] \n",
      "\n",
      "Test Data:\n",
      "\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      " ************************************************************\n",
      "\n",
      " ************************************************************ \n",
      " Generating new features !\n",
      " ************************************************************\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 4186 features.\n",
      "[AutoFeat] With 506 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 2594 feature combinations from 2628 original feature tuples - done.\n",
      "[feateng] Generated altogether 2654 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1199 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Memmapping (shape=(989, 506), dtype=float32) to new file C:\\Users\\CHIENH~1\\AppData\\Local\\Temp\\joblib_memmapping_folder_14756_6903585667\\14756-2521423853512-b1c3070420584d659cce3d2533634b36.pkl\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Pickling array (shape=(506,), dtype=float64).\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Memmapping (shape=(989, 506), dtype=float32) to old file C:\\Users\\CHIENH~1\\AppData\\Local\\Temp\\joblib_memmapping_folder_14756_6903585667\\14756-2521423853512-b1c3070420584d659cce3d2533634b36.pkl\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Pickling array (shape=(506,), dtype=float64).\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Memmapping (shape=(989, 506), dtype=float32) to old file C:\\Users\\CHIENH~1\\AppData\\Local\\Temp\\joblib_memmapping_folder_14756_6903585667\\14756-2521423853512-b1c3070420584d659cce3d2533634b36.pkl\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Pickling array (shape=(506,), dtype=float64).\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Memmapping (shape=(989, 506), dtype=float32) to old file C:\\Users\\CHIENH~1\\AppData\\Local\\Temp\\joblib_memmapping_folder_14756_6903585667\\14756-2521423853512-b1c3070420584d659cce3d2533634b36.pkl\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Pickling array (shape=(506,), dtype=float64).\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Memmapping (shape=(989, 506), dtype=float32) to old file C:\\Users\\CHIENH~1\\AppData\\Local\\Temp\\joblib_memmapping_folder_14756_6903585667\\14756-2521423853512-b1c3070420584d659cce3d2533634b36.pkl\n",
      "Pickling array (shape=(989,), dtype=object).\n",
      "Pickling array (shape=(506,), dtype=float64).\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   29.6s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   30.5s remaining:   45.8s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   30.6s remaining:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   41.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   41.9s finished\n",
      "[featsel] 40 features after 5 feature selection runs\n",
      "[featsel] 30 features after correlation filtering\n",
      "[featsel] 22 features after noise filtering\n",
      "[AutoFeat] Computing 22 new features.\n",
      "[AutoFeat]    22/   22 new features ...done.\n",
      "[AutoFeat] Final dataframe with 35 feature columns (22 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "97.92940801584687\n",
      "-100.721173 * log(x4)/x7\n",
      "59.911380 * 1/(x2*x7)\n",
      "30.503070 * x5**3/x9\n",
      "-12.152309 * sqrt(x0)*x4**3\n",
      "-1.612774 * sqrt(x12)*sqrt(x7)\n",
      "-0.441408 * x12*x4**2\n",
      "0.360673 * x12*x3\n",
      "0.056190 * sqrt(x11)*x5**2\n",
      "-0.038308 * x10**2*sqrt(x12)\n",
      "0.037916 * x0**3*x3\n",
      "0.017533 * x12**3/x6\n",
      "0.006385 * x5**3*log(x7)\n",
      "-0.005686 * sqrt(x11)*x12\n",
      "-0.003027 * x12*x5**3\n",
      "-0.002591 * x6**2/x8\n",
      "-0.000167 * x0*exp(x5)\n",
      "-0.000089 * x9**2/x8\n",
      "-0.000019 * x8**2*exp(x5)\n",
      "[AutoFeat] Final score: 0.8804\n",
      "\n",
      " ************************************************************ \n",
      "Successfully generated new features! and selected the best features\n",
      "\n",
      " X:\n",
      "\n",
      "            0     1      2    3      4      5     6       7    8      9  ...  \\\n",
      "0    0.00632  18.0   2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  ...   \n",
      "1    0.02731   0.0   7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  ...   \n",
      "2    0.02729   0.0   7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  ...   \n",
      "3    0.03237   0.0   2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  ...   \n",
      "4    0.06905   0.0   2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  ...   \n",
      "..       ...   ...    ...  ...    ...    ...   ...     ...  ...    ...  ...   \n",
      "501  0.06263   0.0  11.93  0.0  0.573  6.593  69.1  2.4786  1.0  273.0  ...   \n",
      "502  0.04527   0.0  11.93  0.0  0.573  6.120  76.7  2.2875  1.0  273.0  ...   \n",
      "503  0.06076   0.0  11.93  0.0  0.573  6.976  91.0  2.1675  1.0  273.0  ...   \n",
      "504  0.10959   0.0  11.93  0.0  0.573  6.794  89.3  2.3889  1.0  273.0  ...   \n",
      "505  0.04741   0.0  11.93  0.0  0.573  6.030  80.8  2.5050  1.0  273.0  ...   \n",
      "\n",
      "     1/(x2*x7)  x8**2*exp(x5)  x12*x3  x0*exp(x5)  sqrt(x11)*x12  \\\n",
      "0     0.105844     716.945624     0.0    4.531096      99.213299   \n",
      "1     0.028476    2458.469697     0.0   16.785202     182.090272   \n",
      "2     0.028476    5277.956953     0.0   36.008861      79.874356   \n",
      "3     0.075668    9849.978755     0.0   35.427090      58.403971   \n",
      "4     0.075668   11432.604285     0.0   87.713481     106.186122   \n",
      "..         ...            ...     ...         ...            ...   \n",
      "501   0.033818     729.967491     0.0   45.717864     191.453790   \n",
      "502   0.036644     454.864694     0.0   20.591725     180.894931   \n",
      "503   0.038672    1070.627281     0.0   65.051314     112.362050   \n",
      "504   0.035088     892.476337     0.0   97.806482     128.534520   \n",
      "505   0.033462     415.715029     0.0   19.709050     156.988112   \n",
      "\n",
      "     sqrt(x0)*x4**3    x12*x5**3  x12*x4**2  x5**3*log(x7)  sqrt(x12)*sqrt(x7)  \n",
      "0          0.012380  1415.521970   1.441431     400.366737            4.513114  \n",
      "1          0.017048  2419.659222   2.010444     424.323558            6.737900  \n",
      "2          0.017042  1494.807828   0.886443     594.524082            4.474082  \n",
      "3          0.017285  1007.555887   0.616706     617.581302            4.221714  \n",
      "4          0.025245  1945.801596   1.118042     657.875436            5.684323  \n",
      "..              ...          ...        ...            ...                 ...  \n",
      "501        0.047082  2771.249969   3.174941     260.128919            4.895719  \n",
      "502        0.040028  2081.326026   2.981227     189.671039            4.557466  \n",
      "503        0.046374  1914.690223   1.851776     262.616205            3.496384  \n",
      "504        0.062280  2032.130684   2.127572     273.093592            3.934472  \n",
      "505        0.040964  1727.739069   2.587233     201.340523            4.442904  \n",
      "\n",
      "[506 rows x 35 columns] \n",
      "\n",
      "Test Data:\n",
      "\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      " ************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X,y = load_boston(return_X_y=True)\n",
    "\n",
    "gml = Ghalat_Machine_Learning()\n",
    "new_X,y = gml.Auto_Feature_Engineering(X,y,type_of_task='Regression',test_data=None,\n",
    "                                                          splits=6,fill_na_='median',ratio_drop=0.2,\n",
    "                                                          generate_features=True,feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  LassoLarsCV  got validation loss of  15.22574378266505\n",
      "Model  LinearRegression  got validation loss of  13.928396898829169\n",
      "Model  SVR  got validation loss of  38.99078564542461\n",
      "Model  DecisionTreeRegressor  got validation loss of  26.321973684210523\n",
      "Model  KNeighborsRegressor  got validation loss of  23.689007894736843\n",
      "Model  SGDRegressor  got validation loss of  14.405095681479652\n",
      "Model  RandomForestRegressor  got validation loss of  19.631378794590642\n",
      "Model  AdaBoostRegressor  got validation loss of  18.951907894736845\n",
      "Model  ExtraTreesRegressor  got validation loss of  17.228675299707604\n",
      "[23:32:05] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "Model  XGBRegressor  got validation loss of  17.474874544870907\n",
      "Model  LGBMRegressor  got validation loss of  16.71310839288692\n",
      "Model  CatBoostRegressor  got validation loss of  15.435301674406835\n",
      "Model  GradientBoostingRegressor  got validation loss of  17.498863937354987\n",
      "Model  NaiveBayesianRidge  got validation loss of  14.107349379003507\n",
      "Model  MLPRegressor  got validation accuracy of  21.590957130152546\n",
      "\n",
      " **************************************** \n",
      "Training Neural Network\n",
      " ****************************************\n",
      "Neural Network got validation loss of  22.39757000729242\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               9216      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 50,433\n",
      "Trainable params: 50,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      " ************************************************************ \n",
      "Round One Results\n",
      " ************************************************************ \n",
      "                        Model  Validation_Loss\n",
      "0           LinearRegression        13.928397\n",
      "0         NaiveBayesianRidge        14.107349\n",
      "0               SGDRegressor        14.405096\n",
      "0                LassoLarsCV        15.225744\n",
      "0          CatBoostRegressor        15.435302\n",
      "0              LGBMRegressor        16.713108\n",
      "0        ExtraTreesRegressor        17.228675\n",
      "0               XGBRegressor        17.474875\n",
      "0  GradientBoostingRegressor        17.498864\n",
      "0          AdaBoostRegressor        18.951908\n",
      "0      RandomForestRegressor        19.631379\n",
      "0               MLPRegressor        21.590957\n",
      "0             Neural Network        22.397570\n",
      "0        KNeighborsRegressor        23.689008\n",
      "0      DecisionTreeRegressor        26.321974\n",
      "0                        SVR        38.990786 \n",
      " ************************************************************\n",
      "Model  LinearRegression  got validation accuracy of  15.45483102013546\n",
      "Model  NaiveBayesianRidge  got validation accuracy of  13.060391753780117\n",
      "Model  SGDRegressor  got validation accuracy of  9.92813255686668\n",
      "Model  LassoLarsCV  got validation accuracy of  11.77920256444492\n",
      "Model  CatBoostRegressor  got validation accuracy of  12.130473063242484\n",
      "\n",
      " ************************************************************ \n",
      "Round Two Results\n",
      " ************************************************************ \n",
      "                 Model  Val_Accuracy\n",
      "0        SGDRegressor      9.928133\n",
      "0         LassoLarsCV     11.779203\n",
      "0   CatBoostRegressor     12.130473\n",
      "0  NaiveBayesianRidge     13.060392\n",
      "0    LinearRegression     15.454831 \n",
      " ************************************************************\n",
      "\n",
      "\n",
      " **************************************** \n",
      "Suggested Models for Stacking\n",
      " **************************************** \n",
      " 0         SGDRegressor\n",
      "0          LassoLarsCV\n",
      "0    CatBoostRegressor\n",
      "Name: Model, dtype: object\n",
      "**************************************** \n",
      " PLEASE NOTE: these results are calculated using  <function mean_squared_error at 0x0000024B7EAA0708>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "best_model = gml.GMLRegressor(new_X,y,neural_net='Yes',epochs=100,models=[MLPRegressor()],verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
